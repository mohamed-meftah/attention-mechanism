{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfLf8gpzCf9b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import(Embedding, LSTM, Dense, Input, Layer , Dropout,GlobalAveragePooling1D)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "jlLJNoC3ClrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import (Embedding, LSTM, Dense, Input, Layer, Dropout,\n",
        "                                     GlobalAveragePooling1D)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "max_features = 2000\n",
        "max_len = 200\n",
        "emdediding_dim = 123\n",
        "\n",
        "# charge dataset\n",
        "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=max_features)\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "\n",
        "# define attention layer\n",
        "\n",
        "class Attenttion(Layer):\n",
        "    def __init__(self):\n",
        "        super(Attenttion, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
        "                                 initializer=\"random_normal\",\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=\"zeros\",\n",
        "                                 trainable=True)\n",
        "        self.u = self.add_weight(shape=(input_shape[-1], 1),\n",
        "                                 initializer=\"random_normal\",\n",
        "                                 trainable=True)\n",
        "\n",
        "        super(Attenttion, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # calculate score attention\n",
        "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b) # Changed 'input' to 'inputs'\n",
        "        attention_weights = tf.nn.softmax(tf.tensordot(score, self.u, axes=1), axis=1)\n",
        "        # calcule du contex pondere\n",
        "        context_vector = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "# constract model with LSTM and attention\n",
        "\n",
        "def build_model():\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    x = Embedding(max_features, emdediding_dim)(inputs)\n",
        "    x = LSTM(128, return_sequences=True)(x)  # output of Attention sequence\n",
        "    context_vector, attention_weights = Attenttion()(x)\n",
        "    x = Dropout(0.5)(context_vector)\n",
        "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# compile model\n",
        "model = build_model()\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=64, epochs=5,\n",
        "                    validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "DnK1GF0tDi0N",
        "outputId": "0bba05da-a726-40b0-cc5b-c85d9582dc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m123\u001b[0m)            │         \u001b[38;5;34m246,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m129,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ attenttion_1 (\u001b[38;5;33mAttenttion\u001b[0m)            │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m,   │          \u001b[38;5;34m16,640\u001b[0m │\n",
              "│                                      │ \u001b[38;5;34m1\u001b[0m)]                         │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">123</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">246,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">129,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ attenttion_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attenttion</span>)            │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
              "│                                      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]                         │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m391,793\u001b[0m (1.49 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">391,793</span> (1.49 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m391,793\u001b[0m (1.49 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">391,793</span> (1.49 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.6760 - loss: 0.5609 - val_accuracy: 0.8652 - val_loss: 0.3131\n",
            "Epoch 2/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.8781 - loss: 0.2985 - val_accuracy: 0.8756 - val_loss: 0.2879\n",
            "Epoch 3/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.8988 - loss: 0.2532 - val_accuracy: 0.8777 - val_loss: 0.2853\n",
            "Epoch 4/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9075 - loss: 0.2308 - val_accuracy: 0.8796 - val_loss: 0.2803\n",
            "Epoch 5/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9189 - loss: 0.2093 - val_accuracy: 0.8810 - val_loss: 0.2864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Evaluate model\n",
        "test_loss,test_acc=model.evaluate(X_test,Y_test)\n",
        "print(\"Test Accuracy:\",test_acc)\n",
        "\n",
        "#visuliaze attention weights\n",
        "def visulize_attention(input_sequence,word_index):\n",
        "  reverse_word_index={v:k for k,v in word_index.items()}\n",
        "  words=[reverse_word_index.get(i,'?') for i in input_sequence]\n",
        "\n",
        "  input_sequence= pad_sequences([input_sequence],maxlen=max_len)\n",
        "  model_selection= Model(inputs=model.input,outputs=model.get_layer('attenttion_1').output)\n",
        "  contex_vector,attention_weights=model_selection.predict(input_sequence)\n",
        "  attention_weights=attention_weights.squeeze()\n",
        "  #print word and their words\n",
        "  for word , weight in zip(words,attention_weights):\n",
        "    print(f\"{word}:{weight}\")\n",
        "\n",
        "sample_index=0\n",
        "visulize_attention(X_train[sample_index],word_index=imdb.get_word_index())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GqpcnKPHL9V",
        "outputId": "65d8d733-3a7c-4178-e5f8-5ba8bcd70677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8799 - loss: 0.2885\n",
            "Test Accuracy: 0.8810399770736694\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
            "to:0.0025466508232057095\n",
            "have:0.0030701318755745888\n",
            "after:0.0012352573685348034\n",
            "out:0.0017693248810246587\n",
            "atmosphere:0.0022786774206906557\n",
            "never:0.0024462032597512007\n",
            "more:0.0019163611577823758\n",
            "room:0.007899800315499306\n",
            "and:0.0028005572967231274\n",
            "it:0.002269358141347766\n",
            "so:0.0026202485896646976\n",
            "heart:0.012132992967963219\n",
            "shows:0.0034025325439870358\n",
            "to:0.0026308640372008085\n",
            "years:0.0017241091700270772\n",
            "of:0.0011503141140565276\n",
            "every:0.002774552907794714\n",
            "never:0.00269005517475307\n",
            "going:0.008184238336980343\n",
            "and:0.002833239734172821\n",
            "help:0.0016428310191258788\n",
            "moments:0.0012589169200509787\n",
            "or:0.0008862106478773057\n",
            "of:0.0007711134967394173\n",
            "every:0.0018109593074768782\n",
            "and:0.0012873192317783833\n",
            "visual:0.010828280821442604\n",
            "movie:0.004637171979993582\n",
            "except:0.005061938893049955\n",
            "her:0.0042901416309177876\n",
            "was:0.003694161307066679\n",
            "several:0.009756202809512615\n",
            "of:0.004078875295817852\n",
            "enough:0.0014379044296219945\n",
            "more:0.0011544740991666913\n",
            "with:0.001106751267798245\n",
            "is:0.0010677056852728128\n",
            "now:0.0011165435425937176\n",
            "and:0.0009577626478858292\n",
            "film:0.0009114565909840167\n",
            "as:0.001368071185424924\n",
            "you:0.0038142239209264517\n",
            "of:0.0022646498400717974\n",
            "mine:0.023121340200304985\n",
            "and:0.009356891736388206\n",
            "unfortunately:0.014110728166997433\n",
            "of:0.006366681773215532\n",
            "you:0.015581509098410606\n",
            "than:0.005605539306998253\n",
            "him:0.037424616515636444\n",
            "that:0.01655036211013794\n",
            "with:0.00725014740601182\n",
            "out:0.006610970012843609\n",
            "themselves:0.04865221679210663\n",
            "her:0.03742900863289833\n",
            "get:0.02005944959819317\n",
            "for:0.0075561003759503365\n",
            "was:0.005117315333336592\n",
            "camp:0.0031661083921790123\n",
            "of:0.0019344030879437923\n",
            "you:0.0035647281911224127\n",
            "movie:0.003800345351919532\n",
            "sometimes:0.003980838693678379\n",
            "movie:0.0032365540973842144\n",
            "that:0.0041907052509486675\n",
            "with:0.002628057962283492\n",
            "scary:0.005499758757650852\n",
            "but:0.002248745411634445\n",
            "and:0.0014744381187483668\n",
            "to:0.002219060668721795\n",
            "story:0.0013947237748652697\n",
            "wonderful:0.010110892355442047\n",
            "that:0.006940719671547413\n",
            "in:0.003835165873169899\n",
            "seeing:0.012502443045377731\n",
            "in:0.006645705550909042\n",
            "character:0.011697974056005478\n",
            "to:0.010458373464643955\n",
            "of:0.004866808652877808\n",
            "and:0.0029046162962913513\n",
            "and:0.0022200411185622215\n",
            "with:0.0022541661746799946\n",
            "heart:0.024196064099669456\n",
            "had:0.01617654412984848\n",
            "and:0.006861696485430002\n",
            "they:0.002684343606233597\n",
            "of:0.0021315019112080336\n",
            "here:0.0023155384697020054\n",
            "that:0.003582222620025277\n",
            "with:0.0031788498163223267\n",
            "her:0.005643508862704039\n",
            "serious:0.00704282708466053\n",
            "to:0.0057967836037278175\n",
            "have:0.00749738747254014\n",
            "does:0.004538753069937229\n",
            "when:0.0019451839616522193\n",
            "from:0.001133066019974649\n",
            "why:0.0009765031281858683\n",
            "what:0.0012752561597153544\n",
            "have:0.0029056337662041187\n",
            "critics:0.0072957840748131275\n",
            "they:0.0015741005772724748\n",
            "is:0.0020404590759426355\n",
            "you:0.0042080567218363285\n",
            "that:0.005058552138507366\n",
            "isn't:0.010872289538383484\n",
            "one:0.004935519304126501\n",
            "will:0.0023324040230363607\n",
            "very:0.006601125467568636\n",
            "to:0.004252557642757893\n",
            "as:0.003286134684458375\n",
            "itself:0.02050870843231678\n",
            "with:0.015267985872924328\n",
            "other:0.016034377738833427\n",
            "and:0.0057859416119754314\n",
            "in:0.0050392248667776585\n",
            "of:0.0034776462707668543\n",
            "seen:0.0030278724152594805\n",
            "over:0.0022709500044584274\n",
            "and:0.001377544249407947\n",
            "for:0.001164895948022604\n",
            "anyone:0.0015198902692645788\n",
            "of:0.0010066652903333306\n",
            "and:0.0009213882149197161\n",
            "br:0.0012017914559692144\n",
            "and:0.0011437366483733058\n",
            "to:0.0015240240609273314\n",
            "whether:0.005898660980165005\n",
            "from:0.001737708575092256\n",
            "than:0.001215559896081686\n",
            "out:0.002015336649492383\n",
            "themselves:0.02230418659746647\n",
            "history:0.007138343527913094\n",
            "he:0.003682381473481655\n",
            "name:0.015655411407351494\n",
            "half:0.008203239180147648\n",
            "some:0.007023806218057871\n",
            "br:0.003570166416466236\n",
            "of:0.0017854413017630577\n",
            "and:0.00113204144872725\n",
            "odd:0.0008579919231124222\n",
            "was:0.0010106053669005632\n",
            "two:0.001437711762264371\n",
            "most:0.0019430485554039478\n",
            "of:0.0013738484121859074\n",
            "mean:0.0016370604280382395\n",
            "for:0.0011413868051022291\n",
            "1:0.0008969290065579116\n",
            "any:0.0005907245795242488\n",
            "an:0.0007121045491658151\n",
            "and:0.000782556424383074\n",
            "she:0.0008286571246571839\n",
            "he:0.0013033680152148008\n",
            "should:0.003062221221625805\n",
            "is:0.0030488097108900547\n",
            "thought:0.0018392939819023013\n",
            "and:0.001359481830149889\n",
            "but:0.000892112497240305\n",
            "of:0.0007793777040205896\n",
            "script:0.0020551513880491257\n",
            "you:0.003377243410795927\n",
            "not:0.00139587779995054\n",
            "while:0.0008543449221178889\n",
            "history:0.00110403832513839\n",
            "he:0.0013345436891540885\n",
            "heart:0.010166323743760586\n",
            "to:0.007285990286618471\n",
            "real:0.006665106862783432\n",
            "at:0.0028393305838108063\n",
            "and:0.0016795466654002666\n",
            "but:0.0012362970737740397\n",
            "when:0.0009929632069543004\n",
            "from:0.0007414863212034106\n",
            "one:0.0006776793161407113\n",
            "bit:0.0008007269352674484\n",
            "then:0.0016900888876989484\n",
            "have:0.0022536662872880697\n",
            "two:0.0019069922855123878\n",
            "of:0.0013110967120155692\n",
            "script:0.0022243873681873083\n",
            "their:0.003229705849662423\n",
            "with:0.0029052463360130787\n",
            "her:0.004104998428374529\n",
            "nobody:0.011997767724096775\n",
            "most:0.013099155388772488\n",
            "that:0.008291300386190414\n",
            "with:0.004317894112318754\n",
            "wasn't:0.006935797166079283\n",
            "to:0.00633603660389781\n",
            "with:0.0040410421788692474\n",
            "and:0.0024502186570316553\n",
            "acting:0.002708042971789837\n",
            "watch:0.001917757443152368\n",
            "an:0.00352699076756835\n",
            "for:0.002919045276939869\n",
            "with:0.002338029909878969\n",
            "and:0.001783063169568777\n",
            "film:0.0019330523209646344\n",
            "want:0.005938289221376181\n",
            "an:0.008015424944460392\n"
          ]
        }
      ]
    }
  ]
}
