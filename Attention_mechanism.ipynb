{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sflxuHV-DZPJ"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    (\"love this product\",1),\n",
        "    (\"This in the worst thing eveer\",0),\n",
        "    (\"im so happy with this purchase\",1),\n",
        "    (\"this in not what i expected \",0),\n",
        "    (\"amaziing\",1),\n",
        "    (\"Horrible\",0),\n",
        "    (\"never buying this again\",0),\n",
        "    (\"best thing i got\",1),\n",
        "    (\"i regret buying this\",0),\n",
        "    (\"great quality\",1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7_MponRvEMIq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def tokenize_sentence(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "vocab = Counter()\n",
        "for sentence , label in data:\n",
        "    tokens = tokenize_sentence(sentence)\n",
        "    vocab.update(tokens)\n",
        "\n",
        "word2index = {word : idx for idx ,(word,_) in enumerate(vocab.items(),1)}\n",
        "word2index[\"<PAD>\"] = len(word2index)\n",
        "vocab_size = len(word2index)\n",
        "\n",
        "def encode_sentence(sentence,word2index,max_len=10):\n",
        "    tokens = tokenize_sentence(sentence)\n",
        "    encoded = [word2index.get(token,0) for token in tokens]\n",
        "    encoded = encoded[:max_len]+[0]*(max_len-len(encoded))\n",
        "    return encoded\n",
        "\n",
        "x = [encode_sentence(sentence,word2index) for sentence , label in data]\n",
        "y = [label for sentence , label in data]\n",
        "\n",
        "x_train , x_test, y_train , y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "zH9jR8SYFuCx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self,x,y):\n",
        "        self.x = torch.LongTensor(x)\n",
        "        self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return self.x[idx],self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "train_dataset = SentenceDataset(x_train,y_train)\n",
        "test_dataset = SentenceDataset(x_test,y_test)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=2,shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=2,shuffle=False)\n"
      ],
      "metadata": {
        "id": "lTJG1XluKA26"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self,embed_dim,heads):\n",
        "        super(SelfAttention,self).__init__()\n",
        "        self.query= nn.Linear(embed_dim,embed_dim)\n",
        "        self.key = nn.Linear(embed_dim,embed_dim)\n",
        "        self.value = nn.Linear(embed_dim,embed_dim)\n",
        "        self.scale = embed_dim **0.5\n",
        "\n",
        "    def forward(self,x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        attn_wieghts = F.softmax(torch.matmul(Q,K.T(-2,1))/self.scale,dim=1)\n",
        "        output = torch.matmul(attn_wieghts,V)\n",
        "        return output , attn_wieghts"
      ],
      "metadata": {
        "id": "syS2EgQcK4TF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim,num_classes):\n",
        "        super(SentenceClassifier,self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
        "        self.attention = SelfAttention(embed_dim,heads=2)\n",
        "        self.fc = nn.Linear(embed_dim,num_classes)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        embedded = self.embedding(x)\n",
        "        attention_output , attn_wieghgts = self.attention(embedded)\n",
        "        pooled_output = attention_output.mean(dim=1)\n",
        "        output = self.fc(pooled_output)\n",
        "\n",
        "        return output , attn_wieghgts\n",
        "\n",
        "embed_dim = 16\n",
        "# hidden_dim = 128\n",
        "num_classes = 2\n",
        "model = SentenceClassifier(vocab_size,embed_dim,num_classes)"
      ],
      "metadata": {
        "id": "USdBt3_tO6d1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "#training loop\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch,y_batch in train_loader:\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "Ru_P5hSLRqNw"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}